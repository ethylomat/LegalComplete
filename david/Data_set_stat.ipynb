{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # Regular expressions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm # Shows a smart progress meter - wrap iterable with tqdm(iterable)!\n",
    "import collections \n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import nltk\n",
    "from nltk import bigrams, trigrams, ngrams\n",
    "from nltk.tokenize import word_tokenize \n",
    "# requirement: nltk.download(\"punkt\")\n",
    "import gensim\n",
    "from gensim.summarization.textcleaner import split_sentences\n",
    "from gensim.utils import tokenize\n",
    "from gensim.utils import simple_preprocess \n",
    "import itertools\n",
    "from itertools import chain\n",
    "\n",
    "import codecs\n",
    "from IPython.core.display import HTML\n",
    "plt.style.reload_library()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download(\"stopwords\")\n",
    "# from nltk.corpus import stopwords\n",
    "# stop_words = set(stopwords.words('german'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Later should be improved: Load all data from a Folder and combine it into one data-frame\n",
    "df_raw = pd.read_csv('2020_06_23_CE-BVerwG_DE_Datensatz.csv') \n",
    "df_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data-Set\n",
    "\n",
    "Import the data which sould be used for training and evaluation.\n",
    "\n",
    "Split the data-set into 90/10 (or other wanted ratio), shuffle indices and return random distribution of documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42) \n",
    "def split_dataset(input_text, perc_use, perc_predict): \n",
    "    \"\"\" input_text: the Data-Frame which sould be processed\n",
    "        perc_use: the percentage of data which sould be used for training \n",
    "        perc_predit: \"                                         \" prediction analysis and metrics \n",
    "    \"\"\"\n",
    "    num_doc = len(df_raw) # number of documents\n",
    "    indices_lst = list(range(num_doc)) # list of (for now) ordered indices of the documents\n",
    "    np.random.shuffle(indices_lst) # shuffle the lsit randomly \n",
    "    \n",
    "    # define the data-sets for training and prediciton \n",
    "    df_use = df_raw.iloc[indices_lst[:round(perc_use*num_doc)]][[\"doc_id\", \"Gericht\", \"Entscheidungsart\", \"Verfahrensart\", \"text\"]] # Later use full data-set \n",
    "    df_predict = df_raw.iloc[indices_lst[round((1-perc_predict)*num_doc):]][[\"Gericht\", \"doc_id\", \"text\"]] # Later use full data-set \n",
    "    \n",
    "    # print length of data_frames for validation\n",
    "    print(len(df_use), len(df_predict))\n",
    "    \n",
    "    # reset row index after shuffling, append column with original index\n",
    "    df_use = df_use.reset_index()\n",
    "    df_predict = df_predict.reset_index()\n",
    "    \n",
    "    return df_use, df_predict\n",
    "\n",
    "# Run code to create data-sets\n",
    "df_raw_use, df_raw_predict = split_dataset(df_raw, 1., 0.00) # Later use full data-set \n",
    "#df_raw_use, df_raw_predict = split_dataset(df_raw, 0.9, 0.1) # <- this is the complete data-set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If necessary, only use specific documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Entscheidungsart\": B, U (Beschluss, Urteil)\n",
    "# \"\"Verfahrensart\": B, A, C, P, PB, WNB, PKH, WB, WD, WDS-VR, VR, BN, KSt, AV, F, \n",
    "\n",
    "\n",
    "# df_raw_use.drop(df_raw_use.loc[df_raw_use[\"Entscheidungsart\"] != \"B\"].index, inplace=True)\n",
    "# df_raw_use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting occurrences (helper function)\n",
    "Count occurrences like “VwGO” (or Tokens of interest) in all decisions either explicitly (exact comparison) or implicitly (ignore case): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_occurrence(input_text, word_string, explicit = True): # helper function\n",
    "    count = 0\n",
    "    search = str(word_string)\n",
    "    \n",
    "    lst_found = []\n",
    "    if explicit == True: # search for the exact matching of strings\n",
    "        for decision in tqdm(input_text):\n",
    "            count += len(re.findall(search, decision))\n",
    "            lst_found.append(re.findall(search, decision))\n",
    "    \n",
    "    else:\n",
    "        for decision in tqdm(input_text): # ignore the lower / upper case variance in string\n",
    "            count += len(re.findall(search, decision, re.IGNORECASE))\n",
    "            lst_found.append(re.findall(search, decision))\n",
    "    return count, lst_found\n",
    "    #print('Counts of “%s“: ' % search, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Tokenizer\n",
    "Use a tokenizer for preprocessing of the given data-set.\n",
    "\n",
    "Use \"case_important = True\" for a exact comparison considering the case sensitivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Sentences\n",
    "def tokenizer(text_input, min_length = 2, case_important = False, lan = \"german\"):\n",
    "    \"\"\" text_input: text which should be processed\n",
    "        min_length: minimum length of the tokens which should be returned\n",
    "        case_important: Is upper / lower case from importance (False is much faster)\n",
    "        lan: language of the processed Text\n",
    "    \"\"\"\n",
    "    \n",
    "    if case_important == False:\n",
    "        for instance in range(len(text_input)):\n",
    "            text_input[instance] = simple_preprocess(text_input[instance], min_len = min_length) # use Gensim \n",
    "        \n",
    "    else: # if upper / lower case is from importance\n",
    "        for instance in range(len(text_input)):\n",
    "\n",
    "            sentences = nltk.sent_tokenize(text_input[instance],language=lan) #sentence\n",
    "            sentences_tok = [nltk.tokenize.word_tokenize(sent) for sent in sentences] # Tokenized\n",
    "            accumulated = list(itertools.chain.from_iterable(sentences_tok)) # Merge List\n",
    "            \n",
    "            # Filter the Tokens for short tokens, the list is hereby reversed handeled \n",
    "            [accumulated.pop(i) for i in reversed(range(len(accumulated))) if ((len(accumulated[i]) < min_length) and accumulated[i] != \"§\" and any(xy.isdigit() for xy in accumulated[i]) != True and accumulated[i] != \".\" )]                                                                          \n",
    "                                                                               \n",
    "            accumulated_sent = accumulated    \n",
    "            \n",
    "            text_input[instance] = accumulated_sent # assign tokens back to the Data-frame \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data-set only considering sentences which include key-tokens and tokenize\n",
    "\n",
    "Input: one column of list of text\n",
    "output: column of tokenized sentences includng the key-token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just include sentences with a searchterm = \"§\", then tokenize\n",
    "\n",
    "# same function as above, but only accept searchterm of length one\n",
    "def sentence_reduction_one_it(text_input, searchterm, lang = \"german\", case_i = True, min_len = 2):\n",
    "    \"\"\" text_input: text which will be processed\n",
    "        searchterm: the key-token (in this case only of length one) which must occur within the sentence\n",
    "        lang = \"german\": language of the processed text (default = german)\n",
    "        case_i = True: is the task case sensitive (default = True)\n",
    "        min_len = 2: the minimum length of the tokens (default = 2)\n",
    "        \n",
    "        return: two df columns: 1. processed sentence and 2. split (at searchterm occurance) processed sentence\n",
    "    \"\"\"\n",
    "    k = len(text_input)\n",
    "    text_out = [ [] for _ in range(k)] # gernerate list to write output to\n",
    "    #sentence_split_out = [ [] for _ in range(k)]\n",
    "    \n",
    "    for instance in tqdm(range(k)):\n",
    "        sentences = nltk.sent_tokenize(text_input[instance],language=lang) #sentence\n",
    "        sentences_filtered = [item for item in sentences for i in range(len(item)) if item[i] == searchterm] # just sentences with §\n",
    "        sentences_filtered = list(dict.fromkeys(sentences_filtered))\n",
    "        # here the sentence is filtered and only sntences with § remain\n",
    "        #sentences_filtered = word_tokenize(sentences_filtered)\n",
    "        \n",
    "        #sentences_filtered = [w for w in sentences_filtered if not w in stop_words]  \n",
    "    \n",
    "        \n",
    "#         # second df column [before][searchterm][after], mehr als ein § pro Satz möglich\n",
    "#         sentence_split = [ [] for _ in range(len(sentences_filtered))]\n",
    "#         num_count = 0\n",
    "#         for item in sentences_filtered:\n",
    "#             index_search_lst = [index for index, elem in enumerate(item) if elem == searchterm ] # list of indices per sentence\n",
    "#             new_item = []\n",
    "#             while(len(index_search_lst) != 0):\n",
    "#                 new_item = [[item[0:index_search_lst[-1]]], [searchterm], [item[index_search_lst[-1]+1:]]]\n",
    "#                 del index_search_lst[-1]\n",
    "            \n",
    "#             sentence_split[num_count] = new_item\n",
    "#             num_count += 1\n",
    "        \n",
    "#         for item in sentence_split:\n",
    "#             for kk in item:\n",
    "#                 if kk[0] != searchterm:\n",
    "#                     tokenizer(kk, min_length = min_len, case_important = case_i  )\n",
    "                    \n",
    "                    \n",
    "        #tokenizer(sentences_filtered, min_length = min_len, case_important = case_i  )\n",
    "        \n",
    "        #sentence_split_out[instance] = sentence_split\n",
    "        text_out[instance] = sentences_filtered #list(itertools.chain.from_iterable(sentences_filtered))\n",
    "    #print(len(text_out), len(sentence_split_out))\n",
    "    return text_out#, sentence_split_out "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Co-occurance probability (of single Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/analytics-vidhya/a-comprehensive-guide-to-build-your-own-language-model-in-python-5141b3917d6d\n",
    "\n",
    "# function to create a n-gram from a list of words\n",
    "def get_ngrams(text_input, order, Keep_duplicates = True):\n",
    "    \"\"\" text_input: Text which is given as an input\n",
    "        order: The order of the desired n-gram (bi-gram, tri-gram, ...)\n",
    "        Keep_duplicates: \n",
    "    \"\"\"\n",
    "    for instance in tqdm(range(len(text_input))):\n",
    "        sentence = text_input[instance]\n",
    "        n_grams = ngrams(sentence, order)\n",
    "        output = [ ' '.join(grams) for grams in n_grams]\n",
    "        \n",
    "        if Keep_duplicates == True:\n",
    "            text_input[instance] = output\n",
    "        else:\n",
    "            text_input[instance] = output[::order] #only keep every n-th entry\n",
    "            \n",
    "# function to map probabilities of n-grams following each other (co-occurance)            \n",
    "def n_gram_model_dictionary(text_input, input_ngram_order = 1, mode = \"tri\", Keep_duplic=False):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a placeholder for model\n",
    "    model = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "    \n",
    "    if (input_ngram_order != 1):\n",
    "        get_ngrams(text_input, order=input_ngram_order, Keep_duplicates=Keep_duplic)\n",
    "    \n",
    "    if mode == \"tri\":\n",
    "        # Count frequency of co-occurance, 2 words given, output one\n",
    "        for sentence in tqdm(text_input):\n",
    "            for word1, word2, word3 in trigrams(sentence, pad_right=True, pad_left=True):\n",
    "                model[(word1, word2)][word3] += 1\n",
    "    \n",
    "    else:\n",
    "        # Count frequency of co-occurance  \n",
    "        for sentence in tqdm(text_input):\n",
    "            for word1, word2 in bigrams(sentence, pad_right=True, pad_left=True):\n",
    "                model[word1][word2] += 1\n",
    "         \n",
    "    return model\n",
    "    print(\"Number of dictionary entries:\", len(model))\n",
    "     \n",
    "# Combine dictionaries and look for overlapping keys and combine the entries\n",
    "def accumulate_models(list_of_models): \n",
    "    model_complete = dict()\n",
    "    for i in range(len(list_of_models)):\n",
    "        model_complete = dict(chain(list_of_models[i].items(), model_complete.items()))\n",
    "    return model_complete\n",
    "        \n",
    "def accumulate_models_prob(model_complete):\n",
    "    # Probability of a word, given the previous two words or word (item)\n",
    "    for w1_w2 in tqdm(model_complete):\n",
    "        total_count = float(sum(model_complete[w1_w2].values()))\n",
    "        for word3 in model_complete[w1_w2]:\n",
    "            model_complete[w1_w2][word3] /= total_count\n",
    "    return model_complete\n",
    "\n",
    "\n",
    "# calculate and accumulate all models \n",
    "def model_multi(data_set_col, num_ng_min, num_ng_max, mode_tri = True, mode_bi = True, Keep_duplic=False, col_name = \"text\"):\n",
    "    \"\"\" data_set_col: column of data-set to process\n",
    "        num_ng_min: minimum order of n-gram\n",
    "        num_ng_max: maximum order of n-gram \n",
    "        mode_tri = True: 2 words given one out? (defalult = True)\n",
    "        mode_bi = True: one word given one out? (default = True)\n",
    "        Keep_duplic=False: Keep duplicates in n-gram model?\n",
    "        col_name = \"text\": if col-name of data-frame not calles \"text\", then state it expicitly\n",
    "    \"\"\"\n",
    "    \n",
    "    model_comp = {}\n",
    "    \n",
    "    if mode_tri == True:\n",
    "        for i in tqdm(range(num_ng_min,num_ng_max+1)): \n",
    "            df_preproc_ngram = data_set_col.copy()\n",
    "            model = n_gram_model_dictionary(df_preproc_ngram[col_name], input_ngram_order=i, mode= \"tri\", Keep_duplic=False)\n",
    "            del df_preproc_ngram \n",
    "\n",
    "            model_comp = accumulate_models([model_comp,model])\n",
    "            del model\n",
    "            \n",
    "    if mode_bi == True:\n",
    "        for i in tqdm(range(num_ng_min,num_ng_max+1)): \n",
    "            df_preproc_ngram = data_set_col.copy()\n",
    "            model = n_gram_model_dictionary(df_preproc_ngram[col_name], input_ngram_order=i, mode= \"bi\", Keep_duplic=False)\n",
    "            del df_preproc_ngram \n",
    "\n",
    "            model_comp = accumulate_models([model_comp,model])\n",
    "            del model\n",
    "\n",
    "    accumulate_models_prob(model_comp)  \n",
    "    return model_comp\n",
    "\n",
    "\n",
    "# function to output suggestions of words which should follow the two suggested input-words    \n",
    "def n_gram_get_suggestion(input_dictionary, suggestion1, suggestion2 = None, num_sugg = 3 ):\n",
    "    \"\"\" input_dictionary: dictionary in which the result should be look up\n",
    "        suggestion1: first word \n",
    "        suggestion2: second word, if None, then return bi-gram model suggestion\n",
    "        num_sugg = 3: number of output pairs (default = 3, 0 = all possible pairs)\n",
    "        \n",
    "        returns: ordered by value list of suggested words which are in the dictionary\n",
    "    \"\"\"\n",
    "    \n",
    "    # Problem of overwriting overlapping keys solved! --- accumulate first, then calcualte probability!\n",
    "    \n",
    "    # If suggestion2 is present\n",
    "    if suggestion2 != None:\n",
    "        sorted_toup = sorted(input_dictionary[suggestion1, suggestion2].items(), key = lambda x: x[1], reverse=True)\n",
    "    else: \n",
    "        sorted_toup = sorted(input_dictionary[suggestion1].items(), key = lambda x: x[1], reverse=True)\n",
    "        \n",
    "    if num_sugg == 0:\n",
    "        result = [[k, v] for k, v in sorted_toup]\n",
    "    else:\n",
    "        result = [[k, v] for k, v in sorted_toup][:num_sugg]\n",
    "   # result = sorted(input_dictionary[suggestion1, suggestion2].items(), key = lambda x: x[1], reverse=True)[:num_sugg]\n",
    "            \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search for every Sentence with a key-token (§) and cut the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_regex = df_raw_use.copy() # copy the corresponding raw row to it \n",
    "df_regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_regex = pd.DataFrame(columns = [\"Doc_id\", \"Court\", \"Book\", \"Section\", \"Paragraph\", \"Doc_index\", \"Sent\",\"Sent_comp\" \"Clausel\"])\n",
    "\n",
    "\"\"\" Output should be a pd.Dataframe with colums:\n",
    "    Document id: Where to find the found reference or string occurrence\n",
    "    Gericht: Which court was the case assigned to\n",
    "    Book: Which book of legislation (if not found return None)\n",
    "    Section: Which section within the book (return 0 if non-existent)\n",
    "    Paragraph: --\n",
    "    Document index: Index of Clausel (Should be a tupel (start : end))\n",
    "    sent_comp: complete processed sentence\n",
    "    sent_split: sentence splittet at searchterm\n",
    "    Extracted Clausel: The Clausel of length n which was found to be of interest\n",
    "\"\"\"\n",
    "\n",
    "df_regex = df_raw_use.copy() # copy the corresponding raw row to it \n",
    "\n",
    "df_regex.rename(columns={\"index\":\"Index\", \"Gericht\":\"Court\", \"doc_id\":\"Doc_id\", \"text\":\"Sent\"}, inplace = True)\n",
    "#df_regex = df_regex[[\"Index\", \"Doc_id\", \"Court\", \"Sent_comp\" ]]\n",
    "#df_regex[[\"Sent_comp\"]] = df_raw_use[[\"index\"]]\n",
    "# add sent_comp to df_regex, and doc_id\n",
    "\n",
    "# compute reduction of sentences and add to data-frame\n",
    "df_regex[\"Sent_comp\"] = sentence_reduction_one_it(df_regex[\"Sent\"], '§', lang = \"german\",  case_i = True, min_len = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify reference (court, book,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_int(string: str) -> int:\n",
    "    if string == '':\n",
    "        return None\n",
    "    else:\n",
    "        return int(string)\n",
    "\n",
    "    # Mapping of every sentence which icludes a §\n",
    "def find_references(decision_text: str, book: str) -> list:\n",
    "    row_lsit = []\n",
    "    for instance in tqdm(range(len(decision_text))): # for each document  \n",
    "        row_lsit_2 = []\n",
    "        for k in range(len(decision_text[instance])):\n",
    "\n",
    "            references = re.findall(r'§ (\\d+)\\W*[Abs.]*\\W*(\\d+)*\\W*[S.]*\\W*(\\d+)*\\W*' + book, decision_text[instance][k])\n",
    "            references = [{'book': book,'section': str_int(m[0]), 'paragraph': str_int(m[1])} for m in references]\n",
    "            row_lsit_2.append(references)\n",
    "        row_lsit.append([x for x in row_lsit_2 if x])\n",
    "    return row_lsit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CreateList of regex seach patterns to find all references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_booklst = [\"AdVermiG\", \"AFG\", \"AltPflG\", \"AO\", \"AuslG\", \"BAFöG\", \"BBiG\", \"BDSG\", \"BErzGG\", \"BetrVG\", \"BGB\", \"BGG\", \"BGleiG\", \"BKGG\", \"BRRG\", \"BSeuchG\", \"BSHG\", \"BtBG\", \"BtG\", \"BtmG\", \"BVG\", \"EStG\", \"FEVG\", \"FGG\", \"FÖJG\", \"FSJG\", \"GeschlKrG\", \"GewO\", \"GG\", \"GlG\", \"HeimBSG\", \"HeimG\", \"JArbSchG\", \"JGG\", \"JÖSchG\", \"KHG\", \"KJHG\", \"KrPflG\", \"KSchG\", \"LGBG\", \"MuSchG\", \"PersAuswG\", \"PersVG\", \"PQsG\", \"PsychThG\", \"SchKG\", \"SchwbG\", \"SGB\", \"SGG\", \"StBauFG\", \"StGB\", \"StPO\", \"StVollzG\", \"USG\", \"UVG\", \"VwGO\", \"VwVfG\", \"WoGG\", \"ZDG\", \"ZPO\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_n = len(df_regex[\"Sent_comp\"])\n",
    "references = find_references(df_regex[\"Sent_comp\"].replace('\\n', ''), \"VwGO\")\n",
    "for item in tqdm(ref_booklst):\n",
    "    strng = str(item)\n",
    "    ref = find_references(df_regex[\"Sent_comp\"].replace('\\n', ''), strng)    \n",
    "    for i in range(len_n):\n",
    "        references[i].append(ref[i])\n",
    "        \n",
    "for i in tqdm(range(len_n)): # for each document \n",
    "    references[i] = [x for x in references[i] if x] # delete all empty lists\n",
    "    references[i] = list(itertools.chain(*references[i]))\n",
    "    ref = []\n",
    "    for j in range(len(references[i])):\n",
    "        if len(references[i][j]) == 3:\n",
    "            ref.append([references[i][j]])\n",
    "        else: \n",
    "            ref.append(list(itertools.chain(references[i][j])))\n",
    "    references[i] = list(itertools.chain(*ref))\n",
    "    \n",
    "    \n",
    "# filter unapcked lists\n",
    "for i in tqdm(range(len_n)): # for each document \n",
    "    for j in range(len(references[i])):\n",
    "        if type(references[i][j]) != dict:\n",
    "            references[i][j] = {}\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refer_zwisch = references"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create co-occurence matrix\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trnslate dict to str\n",
    "\n",
    "for i in range(len(references)):\n",
    "    for j in range(len(references[i])):\n",
    "        references[i][j] = [(k,v) for k,v in references[i][j].items()]\n",
    "        references[i][j] = list(itertools.chain(*references[i][j]))\n",
    "        #references[i][j] = list(itertools.chain(*references[i][j]))\n",
    "        references[i][j] = [x for x in references[i][j] if x] # delete all empty lists\n",
    "    references[i] = [x for x in references[i] if x] # delete all empty lists\n",
    "    \n",
    "all_ref = []\n",
    "for i in range(len(references)):\n",
    "    for j in range(len(references[i])):\n",
    "        all_ref.append([str(references[i][j][1]) + str(\" \") + str(references[i][j][3])])\n",
    "        if len(references[i][j]) <= 5:\n",
    "            references[i][j] = (str(references[i][j][1]) + str(\" \") + str(references[i][j][3])) #+ str(references[i][j][5]))\n",
    "        else: \n",
    "            references[i][j] = (str(references[i][j][1]) + str(\" \") + str(references[i][j][3]) + str(\" \") + str(references[i][j][5]))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for all possible entries in referneces (col_names)\n",
    "name_set = set()\n",
    "for i in range(len(references)):\n",
    "    b = set(references[i]) \n",
    "    name_set = name_set | b\n",
    "    \n",
    "    \n",
    "name_set_all = set()\n",
    "for i in range(len(all_ref)):\n",
    "    b = set(all_ref[i]) \n",
    "    name_set_all = name_set_all | b\n",
    "\n",
    "    \n",
    "len(name_set),len(name_set_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "document = references # all data\n",
    "names = name_set\n",
    "\n",
    "occurrences = OrderedDict((name, OrderedDict((name, 0) for name in names)) for name in names)\n",
    "\n",
    "# Find the co-occurrences:\n",
    "for l in document:\n",
    "    for i in range(len(l)):\n",
    "        for item in l[:i] + l[i + 1:]:\n",
    "            occurrences[l[i]][item] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write co_occurrence matrix to csv\n",
    "df_co_occ_complete = pd.DataFrame(occurrences)  \n",
    "df_co_occ_complete.to_csv('co_occurrence_matrix.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create df with 5 most occuring co-occurencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co_occ_lst = []\n",
    "co_occ_lst_name = []\n",
    "for i in range(len(name_set)):\n",
    "    arb = n_gram_get_suggestion(occurrences, suggestion1 = list(name_set)[i],suggestion2 = None, num_sugg = 5 )\n",
    "    co_occ_lst_name.append(list(name_set)[i])\n",
    "    co_occ_lst.append(arb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write co_occurrences per reference to csv\n",
    "df_co_occ = pd.DataFrame(co_occ_lst, columns = [\"First\",\"Second\",\"Third\",\"Fourth\",\"Fifth\"])  \n",
    "df_co_occ.index = co_occ_lst_name\n",
    "df_co_occ.to_csv('5_most_occurrences_per_ref.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which references are the most frequent (in general), which "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for item in list(name_set)[0], seach references(accumulated)\n",
    "all_references = list(itertools.chain(*references))\n",
    "all_ref = list(itertools.chain(*all_ref))\n",
    "all_ref_count = []\n",
    "reference_count = []\n",
    "\n",
    "for i in tqdm(range(len(list(name_set)))):\n",
    "    a,b = search_occurrence(all_references, list(name_set)[i], explicit = True)\n",
    "    reference_count.append([list(name_set)[i], a ])\n",
    "    \n",
    "for i in range(len(name_set_all)):\n",
    "    a,b = search_occurrence(all_ref, list(name_set_all)[i], explicit = True)\n",
    "    all_ref_count.append([list(name_set_all)[i], a ])\n",
    "    \n",
    "reference_count.sort(key = lambda x: x[1]) \n",
    "reference_count.reverse()\n",
    "\n",
    "all_ref_count.sort(key = lambda x: x[1]) \n",
    "all_ref_count.reverse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_length_of_Chart = 50\n",
    "\n",
    "labels = [item[0] for item in reference_count[:max_length_of_Chart]]\n",
    "counts = [item[1] for item in reference_count[:max_length_of_Chart]]\n",
    "\n",
    "x = np.arange(len(labels))  # the label locations\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "rects1 = ax.bar(x, counts, width, label='References')\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "plt.xticks(rotation='vertical')\n",
    "ax.set_ylabel('times of mentioning')\n",
    "ax.set_title('reference')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "def autolabel(rects):\n",
    "    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate('{}'.format(height),\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "\n",
    "autolabel(rects1)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.savefig(\"references_all.jpeg\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length_of_Chart = 50\n",
    "\n",
    "labels = [item[0] for item in all_ref_count[:max_length_of_Chart]]\n",
    "counts = [item[1] for item in all_ref_count[:max_length_of_Chart]]\n",
    "\n",
    "x = np.arange(len(labels))  # the label locations\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "rects1 = ax.bar(x, counts, width, label='References')\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "plt.xticks(rotation='vertical')\n",
    "ax.set_ylabel('times of mentioning')\n",
    "ax.set_title('reference')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "def autolabel(rects):\n",
    "    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate('{}'.format(height),\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "\n",
    "autolabel(rects1)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.savefig(\"references_all_reduced.jpeg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Book occurrences (not obvious ones)\n",
    "Aus den 60 gesetzesbüchern, welche treten nach der VWgo noch häufig auf (VeGO ist offensichtlich) welche könnte noch von bedeutung sein\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref_count liste durchgegehn und nur nach den Büchern suchen und nicht nach ganzen strings \n",
    "\n",
    "all_book_count = []\n",
    "\n",
    "\n",
    "for i in tqdm(range(len(list(ref_booklst)))):\n",
    "    a,b = search_occurrence(all_ref, ref_booklst[i], explicit = True)\n",
    "    all_book_count.append([ref_booklst[i], a ])\n",
    "    \n",
    "all_book_count.sort(key = lambda x: x[1]) \n",
    "all_book_count.reverse()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length_of_Chart = 50\n",
    "\n",
    "labels = [item[0] for item in all_book_count[:max_length_of_Chart]]\n",
    "counts = [item[1] for item in all_book_count[:max_length_of_Chart]]\n",
    "\n",
    "x = np.arange(len(labels))  # the label locations\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "rects1 = ax.bar(x, counts, width, label='occurrences of books')\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "plt.xticks(rotation='vertical')\n",
    "ax.set_ylabel('times of mentioning')\n",
    "ax.set_title('reference')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "def autolabel(rects):\n",
    "    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate('{}'.format(height),\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "\n",
    "autolabel(rects1)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.savefig(\"books_all.jpeg\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length_of_Chart = 50\n",
    "\n",
    "labels = [item[0] for item in all_book_count[1:max_length_of_Chart]]\n",
    "counts = [item[1] for item in all_book_count[1:max_length_of_Chart]]\n",
    "\n",
    "x = np.arange(len(labels))  # the label locations\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "rects1 = ax.bar(x, counts, width, label='occurrences of books')\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "plt.xticks(rotation='vertical')\n",
    "ax.set_ylabel('times of mentioning')\n",
    "ax.set_title('reference')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "def autolabel(rects):\n",
    "    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate('{}'.format(height),\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "\n",
    "autolabel(rects1)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.savefig(\"books_all_reduced.jpeg\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mean, variance Reference frequency (all, urteil, Beschluss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refer_zwischen = refer_zwisch.copy()\n",
    "ind_list_Beschluss = df_raw_use[df_raw_use[\"Entscheidungsart\"] == \"B\"].index\n",
    "ind_list_Urteil = df_raw_use[df_raw_use[\"Entscheidungsart\"] == \"U\"].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(refer_zwisch[0])\n",
    "\n",
    "refer_zwischen_Beschluss = [refer_zwischen[item] for item in ind_list_Beschluss]\n",
    "refer_zwischen_Urteil = [refer_zwischen[item] for item in ind_list_Urteil]\n",
    "\n",
    "len_Besch = len(refer_zwischen_Beschluss)\n",
    "len_Urteil = len(refer_zwischen_Urteil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refer_zwischen_Beschluss_num = []\n",
    "refer_zwischen_Urteil_num = []\n",
    "\n",
    "for i in range(len_Besch):\n",
    "    refer_zwischen_Beschluss_num.append(len(refer_zwischen_Beschluss[i]))\n",
    "    \n",
    "for i in range(len_Urteil):\n",
    "    refer_zwischen_Urteil_num.append(len(refer_zwischen_Urteil[i]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refer_zwischen_xxx_num = refer_zwischen_Beschluss_num + refer_zwischen_Urteil_num\n",
    "refer_means = [np.mean(refer_zwischen_xxx_num),np.mean(refer_zwischen_Beschluss_num), np.mean(refer_zwischen_Urteil_num)]\n",
    "refer_vars = [np.var(refer_zwischen_xxx_num),np.var(refer_zwischen_Beschluss_num), np.var(refer_zwischen_Urteil_num)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"complete set\", \"Beschluss\", \"Urteil\"]\n",
    "counts = [ round(elem, ndigits=2) for elem in refer_means ]\n",
    "\n",
    "x = np.arange(len(labels))  # the label locations\n",
    "width = 0.5  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "rects1 = ax.bar(x, counts, width, label='mean reference occurrence')\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "plt.xticks(rotation='vertical')\n",
    "ax.set_ylabel('mean occurrence')\n",
    "ax.set_title('mean reference occurrence per document')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "def autolabel(rects):\n",
    "    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate('{}'.format(height),\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "\n",
    "autolabel(rects1)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.savefig(\"mean_ref_occurrence_per_doc.jpeg\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"complete set\", \"Beschluss\", \"Urteil\"]\n",
    "counts = [ round(elem, ndigits=2) for elem in refer_vars ]\n",
    "\n",
    "x = np.arange(len(labels))  # the label locations\n",
    "width = 0.5  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "rects1 = ax.bar(x, counts, width, label='variance of reference occurrences')\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "plt.xticks(rotation='vertical')\n",
    "ax.set_ylabel('variance of occurrence')\n",
    "ax.set_title('variance of reference occurrence per document')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "def autolabel(rects):\n",
    "    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate('{}'.format(height),\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "\n",
    "autolabel(rects1)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.savefig(\"mean_ref_occurrence_per_doc_variance.jpeg\")\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
