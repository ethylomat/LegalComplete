{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # Regular expressions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm # Shows a smart progress meter - wrap iterable with tqdm(iterable)!\n",
    "import collections \n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import nltk\n",
    "from nltk import bigrams, trigrams, ngrams\n",
    "# requirement: nltk.download(\"punkt\")\n",
    "import gensim\n",
    "from gensim.summarization.textcleaner import split_sentences\n",
    "from gensim.utils import tokenize\n",
    "from gensim.utils import simple_preprocess \n",
    "import itertools\n",
    "from itertools import chain\n",
    "\n",
    "import codecs\n",
    "from IPython.core.display import HTML\n",
    "plt.style.reload_library()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To-Do's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need Gramma rules?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data-Set\n",
    "\n",
    "Import the data which sould be used for training and evaluation.\n",
    "\n",
    "Split the data-set into 90/10 (or other wanted ratio), shuffle indices and return random distribution of documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Later should be improved: Load all data from a Folder and combine it into one data-frame\n",
    "df_raw = pd.read_csv('2020_06_23_CE-BVerwG_DE_Datensatz.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(input_text, perc_use, perc_predict): \n",
    "    \"\"\" input_text: the Data-Frame which sould be processed\n",
    "        perc_use: the percentage of data which sould be used for training \n",
    "        perc_predit: \"                                         \" prediction analysis and metrics \n",
    "    \"\"\"\n",
    "    num_doc = len(df_raw) # number of documents\n",
    "    indices_lst = list(range(num_doc)) # list of (for now) ordered indices of the documents\n",
    "    np.random.shuffle(indices_lst) # shuffle the lsit randomly \n",
    "    \n",
    "    # define the data-sets for training and prediciton \n",
    "    df_use = df_raw.iloc[indices_lst[:round(perc_use*num_doc)]][[\"Gericht\", \"doc_id\", \"text\"]] # Later use full data-set \n",
    "    df_predict = df_raw.iloc[indices_lst[round((1-perc_predict)*num_doc):]][[\"Gericht\", \"doc_id\", \"text\"]] # Later use full data-set \n",
    "    \n",
    "    # print length of data_frames for validation\n",
    "    print(len(df_use), len(df_predict))\n",
    "    \n",
    "    # reset row index after shuffling, append column with original index\n",
    "    df_use = df_use.reset_index()\n",
    "    df_predict = df_predict.reset_index()\n",
    "    \n",
    "    return df_use, df_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4845 242\n"
     ]
    }
   ],
   "source": [
    "# Run code to create data-sets\n",
    "df_raw_use, df_raw_predict = split_dataset(df_raw, 0.2, 0.01) # Later use full data-set \n",
    "#df_raw_use, df_raw_predict = split_dataset(df_raw, 0.9, 0.1) # <- this is the complete data-set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting occurrences (helper function)\n",
    "Count occurrences like “VwGO” (or Tokens of interest) in all decisions either explicitly (exact comparison) or implicitly (ignore case): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_occurrence(input_text, word_string, explicit = True): # helper function\n",
    "    count = 0\n",
    "    search = str(word_string)\n",
    "    \n",
    "    if explicit == True: # search for the exact matching of strings\n",
    "        for decision in tqdm(input_text):\n",
    "            count += len(re.findall(search, decision))\n",
    "    \n",
    "    else:\n",
    "        for decision in tqdm(input_text): # ignore the lower / upper case variance in string\n",
    "            count += len(re.findall(search, decision, re.IGNORECASE))\n",
    "    \n",
    "    print('Counts of “%s“: ' % search, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d741f5e72b464d7abac7155a42226fe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=4845.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Counts of “§.*VwGO“:  19942\n"
     ]
    }
   ],
   "source": [
    "search_occurrence(df_raw_use['text'], \"§.*VwGO\", False) # False = ignore case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Tokenizer\n",
    "Use a tokenizer for preprocessing of the given data-set.\n",
    "\n",
    "Use \"case_important = True\" for a exact comparison considering the case sensitivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Sentences\n",
    "def tokenizer(text_input, min_length = 2, case_important = False, lan = \"german\"):\n",
    "    \"\"\" text_input: text which should be processed\n",
    "        min_length: minimum length of the tokens which should be returned\n",
    "        case_important: Is upper / lower case from importance (False is much faster)\n",
    "        lan: language of the processed Text\n",
    "    \"\"\"\n",
    "    \n",
    "    if case_important == False:\n",
    "        for instance in range(len(text_input)):\n",
    "            text_input[instance] = simple_preprocess(text_input[instance], min_len = min_length) # use Gensim \n",
    "        \n",
    "    else: # if upper / lower case is from importance\n",
    "        for instance in range(len(text_input)):\n",
    "\n",
    "            sentences = nltk.sent_tokenize(text_input[instance],language=lan) #sentence\n",
    "            sentences_tok = [nltk.tokenize.word_tokenize(sent) for sent in sentences] # Tokenized\n",
    "            accumulated = list(itertools.chain.from_iterable(sentences_tok)) # Merge List\n",
    "            \n",
    "            # Filter the Tokens for short tokens, the list is hereby reversed handeled \n",
    "            [accumulated.pop(i) for i in reversed(range(len(accumulated))) if ((len(accumulated[i]) < min_length) and any(xy.isdigit() for xy in accumulated[i]) != True)]                                                                          \n",
    "                                                                               \n",
    "            accumulated_sent = accumulated    \n",
    "            \n",
    "            text_input[instance] = accumulated_sent # assign tokens back to the Data-frame \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data-set only considering sentences which include key-tokens and tokenize\n",
    "\n",
    "Input: one column of list of text\n",
    "output: two columns, \n",
    "\n",
    "            one with the processed sentence \n",
    "            \n",
    "            the second, sentence split at the point of occuring reference. If occuring at the beginning, return lst(none)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verallgemeinerung für alle searchterms ----- zu langsam...\n",
    "\n",
    "\n",
    "# # Just include sentences with a searchterm = \"§\", then tokenize\n",
    "# def sentence_reduction(text_input, searchterm, lang = \"german\", case_i = True, min_len = 2):\n",
    "#     \"\"\" text_input: text which will be processed\n",
    "#         searchterm: the key-token which must occur within the sentence\n",
    "#         lang = \"german\": language of the processed text (default = german)\n",
    "#         case_i = True: is the task case sensitive (default = True)\n",
    "#         min_len = 2: the minimum length of the tokens (default = 2)\n",
    "        \n",
    "#         return: two df columns: 1. processed sentence and 2. split (at searchterm occurance) processed sentence\n",
    "#     \"\"\"\n",
    "#     k = len(text_input)\n",
    "#     pattern = re.compile(searchterm)\n",
    "#     text_out = [ [] for _ in range(k)] # gernerate list to write output to\n",
    "#     for instance in tqdm(range(k)):\n",
    "#         sentences = nltk.sent_tokenize(text_input[instance],language=lang) #sentence\n",
    "#         sentences_filtered = [item for item in sentences for i in range(len(item)) if pattern.search(item) != None] # just sentences with §\n",
    "        \n",
    "#         # second df column [before][searchterm][after]\n",
    "#         for sent in sentences_filtered:\n",
    "#             index_st = pattern.search(sent).span()  #index of searchterm occurance\n",
    "#             if pattern.search(sent) != None:\n",
    "#                 break\n",
    "#             print(index_st)\n",
    "#             sentences_split_filt = [[], [searchterm], []]\n",
    "        \n",
    "        \n",
    "#         #sentences_split_filt = ...\n",
    "    \n",
    "    \n",
    "    \n",
    "#         #tokenizer(sentences_split_filt, min_length = min_len, case_important = case_i  )\n",
    "#         tokenizer(sentences_filtered, min_length = min_len, case_important = case_i  )\n",
    "        \n",
    "#         #split_sent_out[instance] = sentences_split_filt\n",
    "#         text_out[instance] = sentences_filtered\n",
    "        \n",
    "#     return text_out #, split_sent_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "# df_preproc_regex = df_raw_use.copy() # preprocessed reduced Dataset\n",
    "# #print(df_preproc_regex[\"text\"])\n",
    "# df_preproc_regex[\"text\"] = sentence_reduction(df_preproc_regex[\"text\"], \"VwGO\", lang = \"german\", min_len = 1)\n",
    "# print(df_preproc_regex[\"text\"][0])\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just include sentences with a searchterm = \"§\", then tokenize\n",
    "\n",
    "# same function as above, but only accept searchterm of length one\n",
    "def sentence_reduction_one_it(text_input, searchterm, lang = \"german\", case_i = True, min_len = 2):\n",
    "    \"\"\" text_input: text which will be processed\n",
    "        searchterm: the key-token (in this case only of length one) which must occur within the sentence\n",
    "        lang = \"german\": language of the processed text (default = german)\n",
    "        case_i = True: is the task case sensitive (default = True)\n",
    "        min_len = 2: the minimum length of the tokens (default = 2)\n",
    "        \n",
    "        return: two df columns: 1. processed sentence and 2. split (at searchterm occurance) processed sentence\n",
    "    \"\"\"\n",
    "    k = len(text_input)\n",
    "    text_out = [ [] for _ in range(k)] # gernerate list to write output to\n",
    "    sentence_split_out = [ [] for _ in range(k)]\n",
    "    \n",
    "    for instance in tqdm(range(k)):\n",
    "        sentences = nltk.sent_tokenize(text_input[instance],language=lang) #sentence\n",
    "        sentences_filtered = [item for item in sentences for i in range(len(item)) if item[i] == searchterm] # just sentences with §\n",
    "        \n",
    "        # second df column [before][searchterm][after], mehr als ein § pro Satz möglich\n",
    "        sentence_split = [ [] for _ in range(len(sentences_filtered))]\n",
    "        num_count = 0\n",
    "        for item in sentences_filtered:\n",
    "            index_search_lst = [index for index, elem in enumerate(item) if elem == searchterm ] # list of indices per sentence\n",
    "            new_item = []\n",
    "            while(len(index_search_lst) != 0):\n",
    "                new_item = [[item[0:index_search_lst[-1]]], [searchterm], [item[index_search_lst[-1]+1:]]]\n",
    "                del index_search_lst[-1]\n",
    "            \n",
    "            sentence_split[num_count] = new_item\n",
    "            num_count += 1\n",
    "        \n",
    "        for item in sentence_split:\n",
    "            for kk in item:\n",
    "                if kk[0] != searchterm:\n",
    "                    tokenizer(kk, min_length = min_len, case_important = case_i  )\n",
    "                    \n",
    "                    \n",
    "        tokenizer(sentences_filtered, min_length = min_len, case_important = case_i  )\n",
    "        \n",
    "        sentence_split_out[instance] = sentence_split\n",
    "        text_out[instance] = sentences_filtered\n",
    "    print(len(text_out), len(sentence_split_out))\n",
    "    return text_out, sentence_split_out "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Co-occurance probability (of single Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/analytics-vidhya/a-comprehensive-guide-to-build-your-own-language-model-in-python-5141b3917d6d\n",
    "\n",
    "# function to create a n-gram from a list of words\n",
    "def get_ngrams(text_input, order, Keep_duplicates = True):\n",
    "    \"\"\" text_input: Text which is given as an input\n",
    "        order: The order of the desired n-gram (bi-gram, tri-gram, ...)\n",
    "        Keep_duplicates: \n",
    "    \"\"\"\n",
    "    for instance in tqdm(range(len(text_input))):\n",
    "        sentence = text_input[instance]\n",
    "        n_grams = ngrams(sentence, order)\n",
    "        output = [ ' '.join(grams) for grams in n_grams]\n",
    "        \n",
    "        if Keep_duplicates == True:\n",
    "            text_input[instance] = output\n",
    "        else:\n",
    "            text_input[instance] = output[::order] #only keep every n-th entry\n",
    "            \n",
    "# function to map probabilities of n-grams following each other (co-occurance)            \n",
    "def n_gram_model_dictionary(text_input, input_ngram_order = 1, mode = \"tri\", Keep_duplic=False):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a placeholder for model\n",
    "    model = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "    \n",
    "    if (input_ngram_order != 1):\n",
    "        get_ngrams(text_input, order=input_ngram_order, Keep_duplicates=Keep_duplic)\n",
    "    \n",
    "    if mode == \"tri\":\n",
    "        # Count frequency of co-occurance, 2 words given, output one\n",
    "        for sentence in tqdm(text_input):\n",
    "            for word1, word2, word3 in trigrams(sentence, pad_right=True, pad_left=True):\n",
    "                model[(word1, word2)][word3] += 1\n",
    "    \n",
    "    else:\n",
    "        # Count frequency of co-occurance  \n",
    "        for sentence in tqdm(text_input):\n",
    "            for word1, word2 in bigrams(sentence, pad_right=True, pad_left=True):\n",
    "                model[word1][word2] += 1\n",
    "         \n",
    "    return model\n",
    "    print(\"Number of dictionary entries:\", len(model))\n",
    "     \n",
    "# Combine dictionaries and look for overlapping keys and combine the entries\n",
    "def accumulate_models(list_of_models): \n",
    "    model_complete = dict()\n",
    "    for i in range(len(list_of_models)):\n",
    "        model_complete = dict(chain(list_of_models[i].items(), model_complete.items()))\n",
    "    return model_complete\n",
    "        \n",
    "def accumulate_models_prob(model_complete):\n",
    "    # Probability of a word, given the previous two words or word (item)\n",
    "    for w1_w2 in tqdm(model_complete):\n",
    "        total_count = float(sum(model_complete[w1_w2].values()))\n",
    "        for word3 in model_complete[w1_w2]:\n",
    "            model_complete[w1_w2][word3] /= total_count\n",
    "    return model_complete\n",
    "\n",
    "\n",
    "# calculate and accumulate all models \n",
    "def model_multi(num_ng_min, num_ng_max, mode1 = \"tri\", mode2 = \"bi\", Keep_duplic=False):\n",
    "    model_comp = {}\n",
    "    \n",
    "    if mode1 == \"tri\":\n",
    "        for i in tqdm(range(num_ng_max+1)): \n",
    "            df_preproc_ngram = df_preproc[[\"text\"]].copy()\n",
    "            model = n_gram_model_dictionary(df_preproc_ngram['text'], input_ngram_order=i, mode= mode1, Keep_duplic=False)\n",
    "            del df_preproc_ngram \n",
    "\n",
    "            model_comp = accumulate_models([model_comp,model])\n",
    "            del model\n",
    "            \n",
    "    if mode2 == \"bi\":\n",
    "        for i in tqdm(range(num_ng_max+1)): \n",
    "            df_preproc_ngram = df_preproc[[\"text\"]].copy()\n",
    "            model = n_gram_model_dictionary(df_preproc_ngram['text'], input_ngram_order=i, mode= mode2, Keep_duplic=False)\n",
    "            del df_preproc_ngram \n",
    "\n",
    "            model_comp = accumulate_models([model_comp,model])\n",
    "            del model\n",
    "\n",
    "    accumulate_models_prob(model_comp)  \n",
    "    return model_comp\n",
    "\n",
    "\n",
    "# function to output suggestions of words which should follow the two suggested input-words    \n",
    "def n_gram_get_suggestion(input_dictionary, suggestion1, suggestion2 = None, num_sugg = 3 ):\n",
    "    \"\"\" input_dictionary: dictionary in which the result should be look up\n",
    "        suggestion1: first word \n",
    "        suggestion2: second word, if None, then return bi-gram model suggestion\n",
    "        num_sugg = 3: number of output pairs (default = 3, 0 = all possible pairs)\n",
    "        \n",
    "        returns: ordered by value list of suggested words which are in the dictionary\n",
    "    \"\"\"\n",
    "    \n",
    "    # Problem of overwriting overlapping keys solved! --- accumulate first, then calcualte probability!\n",
    "    \n",
    "    # If suggestion2 is present\n",
    "    if suggestion2 != None:\n",
    "        sorted_toup = sorted(input_dictionary[suggestion1, suggestion2].items(), key = lambda x: x[1], reverse=True)\n",
    "    else: \n",
    "        sorted_toup = sorted(input_dictionary[suggestion1].items(), key = lambda x: x[1], reverse=True)\n",
    "        \n",
    "    if num_sugg == 0:\n",
    "        result = [[k, v] for k, v in sorted_toup]\n",
    "    else:\n",
    "        result = [[k, v] for k, v in sorted_toup][:num_sugg]\n",
    "   # result = sorted(input_dictionary[suggestion1, suggestion2].items(), key = lambda x: x[1], reverse=True)[:num_sugg]\n",
    "            \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regular expressions approach (just for §)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search for every Sentence with a key-token (§) and cut the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_regex = pd.DataFrame(columns = [\"Doc_id\", \"Court\", \"Book\", \"Section\", \"Paragraph\", \"Doc_index\", \"Sent_comp\", \"Sent_split\", \"Clausel\"])\n",
    "\n",
    "\"\"\" Output should be a pd.Dataframe with colums:\n",
    "    Document id: Where to find the found reference or string occurrence\n",
    "    Gericht: Which court was the case assigned to\n",
    "    Book: Which book of legislation (if not found return None)\n",
    "    Section: Which section within the book (return 0 if non-existent)\n",
    "    Paragraph: --\n",
    "    Document index: Index of Clausel (Should be a tupel (start : end))\n",
    "    sent_comp: complete processed sentence\n",
    "    sent_split: sentence splittet at searchterm\n",
    "    Extracted Clausel: The Clausel of length n which was found to be of interest\n",
    "\"\"\"\n",
    "\n",
    "df_regex = df_raw_use.copy() # copy the corresponding raw row to it \n",
    "\n",
    "df_regex.rename(columns={\"index\":\"Index\", \"Gericht\":\"Court\", \"doc_id\":\"Doc_id\", \"text\":\"Sent_comp\"}, inplace = True)\n",
    "df_regex = df_regex[[\"Index\", \"Doc_id\", \"Court\", \"Sent_comp\" ]]\n",
    "df_regex[[\"Sent_split\"]] = df_raw_use[[\"index\"]]\n",
    "# add sent_comp to df_regex, and doc_id\n",
    "\n",
    "# compute reduction of sentences and add to data-frame\n",
    "#df_regex[\"Sent_comp\"], df_regex[\"Sent_split\"] = sentence_reduction_one_it(df_regex[\"Sent_comp\"], '§', lang = \"german\",  case_i = True, min_len = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>Doc_id</th>\n",
       "      <th>Court</th>\n",
       "      <th>Sent_comp</th>\n",
       "      <th>Sent_split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23981</td>\n",
       "      <td>BVerwG_2019-12-19_B_7_VR_5_19_NA.txt</td>\n",
       "      <td>BVerwG</td>\n",
       "      <td>BESCHLUSS\\nBVerwG 7 VR 5.19 (7 A 8.19)\\n\\nIn d...</td>\n",
       "      <td>23981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13728</td>\n",
       "      <td>BVerwG_2010-05-12_B_22_WD_8_10_NA.txt</td>\n",
       "      <td>BVerwG</td>\n",
       "      <td>BUNDESVERWALTUNGSGERICHT\\nBESCHLUSS\\nBVerwG 2 ...</td>\n",
       "      <td>13728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5790</td>\n",
       "      <td>BVerwG_2005-04-19_B_8_B_19_05_NA.txt</td>\n",
       "      <td>BVerwG</td>\n",
       "      <td>BUNDESVERWALTUNGSGERICHT\\nBESCHLUSS\\nBVerwG 8 ...</td>\n",
       "      <td>5790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7327</td>\n",
       "      <td>BVerwG_2006-03-22_B_4_BN_9_06_NA.txt</td>\n",
       "      <td>BVerwG</td>\n",
       "      <td>BUNDESVERWALTUNGSGERICHT\\nBESCHLUSS\\nBVerwG 4 ...</td>\n",
       "      <td>7327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14360</td>\n",
       "      <td>BVerwG_2010-10-19_B_4_BN_38_10_NA.txt</td>\n",
       "      <td>BVerwG</td>\n",
       "      <td>BUNDESVERWALTUNGSGERICHT\\nBESCHLUSS\\nBVerwG 4 ...</td>\n",
       "      <td>14360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4840</th>\n",
       "      <td>2768</td>\n",
       "      <td>BVerwG_2003-08-12_B_8_B_91_03_NA.txt</td>\n",
       "      <td>BVerwG</td>\n",
       "      <td>BUNDESVERWALTUNGSGERICHT\\nBESCHLUSS\\nBVerwG 8 ...</td>\n",
       "      <td>2768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4841</th>\n",
       "      <td>1527</td>\n",
       "      <td>BVerwG_2003-01-09_B_3_B_77_02_NA.txt</td>\n",
       "      <td>BVerwG</td>\n",
       "      <td>BUNDESVERWALTUNGSGERICHT\\nBESCHLUSS\\nBVerwG 3 ...</td>\n",
       "      <td>1527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4842</th>\n",
       "      <td>864</td>\n",
       "      <td>BVerwG_2002-08-14_B_7_B_25_02_NA.txt</td>\n",
       "      <td>BVerwG</td>\n",
       "      <td>BUNDESVERWALTUNGSGERICHT\\nBESCHLUSS\\nBVerwG 7 ...</td>\n",
       "      <td>864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4843</th>\n",
       "      <td>16219</td>\n",
       "      <td>BVerwG_2011-12-22_B_2_B_69_11_NA.txt</td>\n",
       "      <td>BVerwG</td>\n",
       "      <td>BUNDESVERWALTUNGSGERICHT\\nBESCHLUSS\\nBVerwG 2 ...</td>\n",
       "      <td>16219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4844</th>\n",
       "      <td>19609</td>\n",
       "      <td>BVerwG_2015-02-26_B_2_C_2_14_NA.txt</td>\n",
       "      <td>BVerwG</td>\n",
       "      <td>BUNDESVERWALTUNGSGERICHT\\nBESCHLUSS\\nBVerwG 2 ...</td>\n",
       "      <td>19609</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4845 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Index                                 Doc_id   Court  \\\n",
       "0     23981   BVerwG_2019-12-19_B_7_VR_5_19_NA.txt  BVerwG   \n",
       "1     13728  BVerwG_2010-05-12_B_22_WD_8_10_NA.txt  BVerwG   \n",
       "2      5790   BVerwG_2005-04-19_B_8_B_19_05_NA.txt  BVerwG   \n",
       "3      7327   BVerwG_2006-03-22_B_4_BN_9_06_NA.txt  BVerwG   \n",
       "4     14360  BVerwG_2010-10-19_B_4_BN_38_10_NA.txt  BVerwG   \n",
       "...     ...                                    ...     ...   \n",
       "4840   2768   BVerwG_2003-08-12_B_8_B_91_03_NA.txt  BVerwG   \n",
       "4841   1527   BVerwG_2003-01-09_B_3_B_77_02_NA.txt  BVerwG   \n",
       "4842    864   BVerwG_2002-08-14_B_7_B_25_02_NA.txt  BVerwG   \n",
       "4843  16219   BVerwG_2011-12-22_B_2_B_69_11_NA.txt  BVerwG   \n",
       "4844  19609    BVerwG_2015-02-26_B_2_C_2_14_NA.txt  BVerwG   \n",
       "\n",
       "                                              Sent_comp  Sent_split  \n",
       "0     BESCHLUSS\\nBVerwG 7 VR 5.19 (7 A 8.19)\\n\\nIn d...       23981  \n",
       "1     BUNDESVERWALTUNGSGERICHT\\nBESCHLUSS\\nBVerwG 2 ...       13728  \n",
       "2     BUNDESVERWALTUNGSGERICHT\\nBESCHLUSS\\nBVerwG 8 ...        5790  \n",
       "3     BUNDESVERWALTUNGSGERICHT\\nBESCHLUSS\\nBVerwG 4 ...        7327  \n",
       "4     BUNDESVERWALTUNGSGERICHT\\nBESCHLUSS\\nBVerwG 4 ...       14360  \n",
       "...                                                 ...         ...  \n",
       "4840  BUNDESVERWALTUNGSGERICHT\\nBESCHLUSS\\nBVerwG 8 ...        2768  \n",
       "4841  BUNDESVERWALTUNGSGERICHT\\nBESCHLUSS\\nBVerwG 3 ...        1527  \n",
       "4842  BUNDESVERWALTUNGSGERICHT\\nBESCHLUSS\\nBVerwG 7 ...         864  \n",
       "4843  BUNDESVERWALTUNGSGERICHT\\nBESCHLUSS\\nBVerwG 2 ...       16219  \n",
       "4844  BUNDESVERWALTUNGSGERICHT\\nBESCHLUSS\\nBVerwG 2 ...       19609  \n",
       "\n",
       "[4845 rows x 5 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_regex[\"Sent_comp\"][1][1], df_regex[\"Sent_split\"][1][1]\n",
    "df_regex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify court, book, ... and add sentence after (end of §...) to clausel\n",
    "\n",
    "--> Goal: create dictionary \n",
    "\n",
    "        {identification (VwGo Abs. x...): 1. most propable clausel of lenght 1\n",
    "                                                                        \n",
    "                                          2. most propable clausel of lenght 2\n",
    "                                                                                                       \n",
    "                                          .....}\n",
    "                                          \n",
    "                                          (or three most propable cluase (given with probability!!!!))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hierarchy = {\n",
    "#     'section': '§ %d ',\n",
    "#     'paragraph': 'Abs. %d ',\n",
    "#     'clause': 'S. %d',\n",
    "# }\n",
    "\n",
    "def str_int(string: str) -> int:\n",
    "    if string == '':\n",
    "        return None\n",
    "    else:\n",
    "        return int(string)\n",
    "    \n",
    "# def reference_to_str(reference_dict: dict, level: int = None, book: str = '') -> str:\n",
    "#     result = ''\n",
    "#     book = ' ' + book.strip()\n",
    "#     n = 0\n",
    "#     for key, expression in hierarchy.items():\n",
    "#         value = reference_dict[key]\n",
    "#         if value is not None and (level is None or n <= level):\n",
    "#             result += expression %  value\n",
    "#         n += 1\n",
    "#     result = result.strip() + book\n",
    "#     result = result.strip()\n",
    "#     return result\n",
    "\n",
    "# def find_references(decision_text: str, book: str) -> list:\n",
    "#     references = re.findall(r'§ (\\d+)\\W*[Abs.]*\\W*(\\d+)*\\W*[S.]*\\W*(\\d+)*\\W*' + book, decision_text.replace('\\n', ''))\n",
    "#     references = [{'book': book,'section': str_int(m[0]), 'paragraph': str_int(m[1]), 'clause': str_int(m[2])} for m in references]\n",
    "#     return references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping of every sentence which icludes a §\n",
    "def find_references(decision_text: str, book: str) -> list:\n",
    "    row_lsit = []\n",
    "    for instance in tqdm(range(len(decision_text))): # for each document  \n",
    "        row_lsit_2 = []\n",
    "        for k in range(len(decision_text[instance])):\n",
    "\n",
    "            references = re.findall(r'§ (\\d+)\\W*[Abs.]*\\W*(\\d+)*\\W*[S.]*\\W*(\\d+)*\\W*' + book, decision_text[instance][k])\n",
    "            references = [{'book': book,'section': str_int(m[0]), 'paragraph': str_int(m[1]), 'clause': str_int(m[2])} for m in references]\n",
    "            row_lsit_2.append(references)\n",
    "        row_lsit.append(row_lsit_2)\n",
    "    return row_lsit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find_references(df_preproc_regex[\"text\"], \"VwGO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### search for Paragraph and add it with n words before and k words after to a list.\n",
    "### Then throw either N-Gram on it and / or count the explicit probabilities for n words after Paragraph occurance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also create df with everythin after and everything before § <- throw n-gram on sentence after §"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a3b4b1e601f475f832024dfaa3a0457",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=2731.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_preproc_ngram = df_regex.copy()\n",
    "model_bigram = n_gram_model_dictionary(df_preproc_ngram['Sent_comp'][3], 1, Keep_duplic=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compare word frequency (of lenght 1....n), given the  mapping to book, section, paragraph....\n",
    "\n",
    "### Alternativ; Then throw tokenizer and n-gram model on it, compute probability\n",
    "\n",
    "### Idea: invese word-sentence frequency (tf-idf), weighted with the probability for better suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Gram approach (for complete Text)\n",
    "\n",
    "Throw N-Gram at the complete Corpus after tokinazation.\n",
    "\n",
    "Use N-Gram for the sentence after a found start of reference. (Combine Regex with N-Gram to save computing cost, more precice solution?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approach: \n",
    "\n",
    "Preprocess corpus (Tokenization, ...), then check for N-Grams.\n",
    "\n",
    "Always retrace to the original sentence. append original sentence instead of reduced form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize and save df as preprocessed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-5e7a0b3cfc05>:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  text_input[instance] = simple_preprocess(text_input[instance], min_len = min_length) # use Gensim\n"
     ]
    }
   ],
   "source": [
    "# Create Copy of Data-set\n",
    "df_preproc = df_raw_use.copy() # preprocessed reduced Dataset\n",
    "\n",
    "# Tokenize Data-set\n",
    "tokenizer(df_preproc['text'], min_length = 2, case_important = False  )\n",
    "#df_preproc['text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem right now:  get_ngrams has the problem; word_overlapping when comparing following words (or ngrams) -> Try to solve later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4654315c37f4ac0932446efd1df344f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d316a3fe1784cf3888a7fed9ec44922",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=4845.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aa0358b45d04bf89e3fb510924bc968",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=4845.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d937d1999d248e994afc78155a8da68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=4845.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "440a3cc10d3a4fccb255232e65bb5e2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "432b1a2ccaf4470ea23ca1e29c318ecf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=4845.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6879c15f32545a399d1ce57d5dab81b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=4845.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ec0c16624474aeaba47afa10e851821",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=4845.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0300825f1b62424ca8b4de57fd033890",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=4337604.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_complete = model_multi(1, 2, mode1 = \"tri\", mode2 = \"bi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['behaupteten erhebt', 0.014218009478672985],\n",
       " ['einer auf', 0.014218009478672985],\n",
       " ['auslegung von', 0.014218009478672985]]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_gram_get_suggestion(model_complete, \"im zusammenhang\", \"mit der\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9632bf9353144ac0b806107acdf1b41d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=4845.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d747982b82c41269be80740496e46a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=4845.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c13a72a8c2864a0d84d6a61b983eda85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=4845.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55e95498d9144728ba55685a04cb6c16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=4845.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1472730d644e45299d74718259231661",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=4845.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a85c3a6b7e947b2986530a0c44fc58d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=4845.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01b4e190a7bb458096f2a8be96a1ce35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=4845.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a565a796bbde45ef9fc5d35a4560618e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=4845.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4630af5823e483da5166d8fb033d670",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=4845.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49996a1320ff4c12b7f1f1edda16a1f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=4845.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f4ac5f344fb46d5af4de89dc05c6de7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=4845.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "462181b98353446099a503719fbf6222",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=4845.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48d1b0f46d6c46cd9b5dee774870968d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=4845.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68feec6010024a38925e33643fc15155",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=4845.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2314ae36f33412fb77c6494d7efeb9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=4845.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9dedcaa9a024a18aca16fb54c86e5cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=4845.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function tqdm.__del__ at 0x0000018E1CD29700>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\David\\anaconda3\\lib\\site-packages\\tqdm\\std.py\", line 1122, in __del__\n",
      "    self.close()\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72784a18878a474aa20d21268e860b26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=4845.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25464fdcc2794a85ae6abaaa18bba6b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=4845.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## creat shorter version of this above!!!\n",
    "df_preproc_ngram = df_preproc[[\"text\"]].copy()\n",
    "model_1gram_tri = n_gram_model_dictionary(df_preproc_ngram['text'], input_ngram_order=1, mode=\"tri\", Keep_duplic=False)\n",
    "del df_preproc_ngram \n",
    "\n",
    "df_preproc_ngram = df_preproc[[\"text\"]].copy()\n",
    "model_2gram_tri = n_gram_model_dictionary(df_preproc_ngram['text'], input_ngram_order=2, mode=\"tri\", Keep_duplic=False)\n",
    "del df_preproc_ngram \n",
    "\n",
    "df_preproc_ngram = df_preproc[[\"text\"]].copy()\n",
    "model_3gram_tri = n_gram_model_dictionary(df_preproc_ngram['text'], input_ngram_order=3, mode=\"tri\", Keep_duplic=False)\n",
    "del df_preproc_ngram \n",
    "\n",
    "df_preproc_ngram = df_preproc[[\"text\"]].copy()\n",
    "model_4gram_tri = n_gram_model_dictionary(df_preproc_ngram['text'], input_ngram_order=4, mode=\"tri\", Keep_duplic=False)\n",
    "del df_preproc_ngram \n",
    "\n",
    "df_preproc_ngram = df_preproc[[\"text\"]].copy()\n",
    "model_5gram_tri = n_gram_model_dictionary(df_preproc_ngram['text'], input_ngram_order=5, mode=\"tri\", Keep_duplic=False)\n",
    "del df_preproc_ngram \n",
    "\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "df_preproc_ngram = df_preproc[[\"text\"]].copy()\n",
    "model_1gram_bi = n_gram_model_dictionary(df_preproc_ngram['text'], input_ngram_order=1, mode=\"bi\", Keep_duplic=False)\n",
    "del df_preproc_ngram \n",
    "\n",
    "df_preproc_ngram = df_preproc[[\"text\"]].copy()\n",
    "model_2gram_bi = n_gram_model_dictionary(df_preproc_ngram['text'], input_ngram_order=2, mode=\"bi\", Keep_duplic=False)\n",
    "del df_preproc_ngram \n",
    "\n",
    "df_preproc_ngram = df_preproc[[\"text\"]].copy()\n",
    "model_3gram_bi = n_gram_model_dictionary(df_preproc_ngram['text'], input_ngram_order=3, mode=\"bi\", Keep_duplic=False)\n",
    "del df_preproc_ngram \n",
    "\n",
    "df_preproc_ngram = df_preproc[[\"text\"]].copy()\n",
    "model_4gram_bi = n_gram_model_dictionary(df_preproc_ngram['text'], input_ngram_order=4, mode=\"bi\", Keep_duplic=False)\n",
    "del df_preproc_ngram \n",
    "\n",
    "df_preproc_ngram = df_preproc[[\"text\"]].copy()\n",
    "model_5gram_bi = n_gram_model_dictionary(df_preproc_ngram['text'], input_ngram_order=5, mode=\"bi\", Keep_duplic=False)\n",
    "del df_preproc_ngram "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gram_get_suggestion(model_2gram_tri,  \"im zusammenhang\", \"mit der\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run for multiple n-grams and combine dictionary!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lst = [model_1gram_tri, model_2gram_tri, model_3gram_tri, model_4gram_tri, model_5gram_tri, model_1gram_bi, model_2gram_bi, model_3gram_bi, model_4gram_bi, model_5gram_bi]\n",
    "\n",
    "model_comp = accumulate_models(model_lst) # with accumulate, also te probabilities are calcualted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gram_get_suggestion(model_comp, \"es sich um\", num_sugg=10), n_gram_get_suggestion(model_comp, \"es sich um\", \"eine dienststelle im\", num_sugg=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To-do\n",
    "mixed n-gram: given 5 words-chain, what is the next suggested word. (update n_gram_model_dictionary, !!! don't screw up the probabilities)\n",
    "\n",
    "Or random, define function with arbitrary input length of words (5 words = 3,2 or 2,3 length) and return probabilities\n",
    "\n",
    "Look up, if full lenght entry are available, if not reduce to shorter entries.\n",
    "\n",
    "\n",
    "-------------- alternativ\n",
    "\n",
    "look at every possible combination, take the most probable entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting\n",
    "\n",
    "Accumulate entires within the df, add accumulation column.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Export DataFrames (raw and accumulated) to .csv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation, Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split every sentence after the occurance of § and write a loop\n",
    "\n",
    "def continious_predict():\n",
    "    for the k next predicted words (n = k/2 of sentence length)\n",
    "    for n-gram of the 4 signs/words before, predict the next two words.\n",
    "    \n",
    "    \n",
    "-----do this for n-grams of length 1 to 3 (just predict one next word to the next three words)\n",
    "-----do this with consideration of only 1 last word......last 4 words\n",
    "\n",
    "------ for this write function of n-grams that is randomly combinable......3 words....predict next word.....given two words predict next word.....\n",
    "\n",
    "\n",
    "(repeat everything for random 1000 sentences within the whole document and check precicion achieved)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the prediction data-set in such a way, that prediction can be undertaken\n",
    "df_raw_predict # the Data-set used for validation, evaluation and testing metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vizualization\n",
    "\n",
    "Plot most occuring sentences: compare N-Gram to Regex approach\n",
    "\n",
    "plot everything in the evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
