{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import collections\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../datasets/raw/2020_06_23_CE-BVerwG_DE_Datensatz.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "minidf = df.sample(frac = 0.05)\n",
    "minidf['text_normalized'] = minidf['text'].str.lower().str.replace(\"\\n\", \" \").replace('\\s+', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = nltk.lm.Vocabulary(minidf['text_normalized'], unk_cutoff=10)\n",
    "data = []\n",
    "for judgement in minidf['text_normalized']:\n",
    "    data.extend(judgement.split(\" \"))\n",
    "filtered_data = [word for word in data if len(word) > 1 or not word.isalpha()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_len = 10\n",
    "max_len = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "longgrams = list(nltk.everygrams(filtered_data, min_len=min_len, max_len=max_len-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common = Counter(longgrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common = most_common.most_common(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = []\n",
    "frequencies_per_ngramlength = {i:[] for i in range(min_len, max_len)}\n",
    "for phrase, frequency in most_common:\n",
    "    if frequency < 5: # stop evaluating if the first ngram has less than x occurences\n",
    "        break\n",
    "    frequencies_per_ngramlength[len(phrase)].append((frequency, phrase))\n",
    "\n",
    "frequencies = []\n",
    "lengths = []\n",
    "length_keys = list(frequencies_per_ngramlength.keys())\n",
    "for length in length_keys[::-1]:\n",
    "    counter = 0\n",
    "    for frequency, phrase_candidate in frequencies_per_ngramlength[length]:\n",
    "        pad_dist = length//3\n",
    "        cut_phrase_candidate = \" \".join(phrase_candidate[pad_dist:-pad_dist])\n",
    "        phrase_candidate = \" \".join(phrase_candidate)\n",
    "\n",
    "        if not any([cut_phrase_candidate in phrase for phrase in phrases]):\n",
    "            lengths.append(length)\n",
    "            frequencies.append(frequency)\n",
    "            phrases.append(phrase_candidate)\n",
    "            counter += 1\n",
    "            if counter > 5: # we only check the top 5 phrases at the given n\n",
    "                break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freq:  79 length 15\n",
      "kläger trägt die kosten des beschwerdeverfahrens. der wert des streitgegenstandes wird für das beschwerdeverfahren auf\n",
      "\n",
      "freq:  76 length 15\n",
      "revision ist innerhalb eines monats nach zustellung dieses beschlusses zu begründen. die begründung ist bei\n",
      "\n",
      "freq:  75 length 15\n",
      "zustellung dieses beschlusses zu begründen. die begründung ist bei dem bundesverwaltungsgericht, simsonplatz 1, 04107 leipzig,\n",
      "\n",
      "freq:  65 length 15\n",
      "die revision wird zugelassen. die entscheidung über die kosten des beschwerdeverfahrens folgt der kostenentscheidung in\n",
      "\n",
      "freq:  62 length 15\n",
      "der einlegung einer revision durch den beschwerdeführer bedarf es nicht. die revision ist innerhalb eines\n",
      "\n",
      "freq:  60 length 15\n",
      "begründung ist bei dem bundesverwaltungsgericht, simsonplatz 1, 04107 leipzig, schriftlich oder in elektronischer form (verordnung\n",
      "\n",
      "freq:  71 length 14\n",
      "beschlossen: die beschwerde des klägers gegen die nichtzulassung der revision in dem urteil des\n",
      "\n",
      "freq:  63 length 14\n",
      "beschwerdeführer bedarf es nicht. die revision ist innerhalb eines monats nach zustellung dieses beschlusses\n",
      "\n",
      "freq:  56 length 14\n",
      "der wert des streitgegenstandes wird für das beschwerdeverfahren auf 5 000 € festgesetzt. gründe:\n",
      "\n",
      "freq:  52 length 14\n",
      "§ 141 satz 1, § 125 abs. 1 satz 1, § 92 abs. 3\n",
      "\n",
      "freq:  52 length 14\n",
      "1, 04107 leipzig, schriftlich oder in elektronischer form (verordnung vom 26. november 2004, bgbl\n",
      "\n",
      "freq:  52 length 14\n",
      "die nichtzulassung der revision in dem urteil des oberverwaltungsgerichts für das land nordrhein-westfalen vom\n",
      "\n",
      "freq:  55 length 13\n",
      "vom 19. august 1997 - bverwg 7 261.97 - buchholz 310 § 133\n",
      "\n",
      "freq:  52 length 13\n",
      "beschlossen: die beschwerde der klägerin gegen die nichtzulassung der revision in dem urteil\n",
      "\n",
      "freq:  51 length 13\n",
      "durch den vorsitzenden richter am bundesverwaltungsgericht dr. und die richter am bundesverwaltungsgericht dr.\n",
      "\n",
      "freq:  49 length 13\n",
      "rechtslehrer an einer deutschen hochschule im sinne des hochschulrahmengesetzes mit befähigung zum richteramt\n",
      "\n",
      "freq:  48 length 13\n",
      "§ 47 abs. 1 satz 1 und abs. 3 i.v.m. § 52 abs.\n",
      "\n",
      "freq:  44 length 13\n",
      "durch einen rechtsanwalt oder einen rechtslehrer an einer deutschen hochschule im sinne des\n",
      "\n",
      "freq:  64 length 12\n",
      "durch den vorsitzenden richter am bundesverwaltungsgericht kley und die richter am bundesverwaltungsgericht\n",
      "\n",
      "freq:  53 length 12\n",
      "auf § 47 abs. 1 und 3, § 52 abs. 1 gkg.\n",
      "\n",
      "freq:  50 length 12\n",
      "durch den vorsitzenden richter am bundesverwaltungsgericht neumann und die richter am bundesverwaltungsgericht\n",
      "\n",
      "freq:  44 length 12\n",
      "durch den vorsitzenden richter am bundesverwaltungsgericht prof. dr. rubel und die richter\n",
      "\n",
      "freq:  43 length 12\n",
      "die kostenentscheidung folgt aus § 154 abs. 2 vwgo. die festsetzung des\n",
      "\n",
      "freq:  42 length 12\n",
      "für die beteiligten besteht vertretungszwang; dies gilt auch für die begründung der\n",
      "\n",
      "freq:  56 length 11\n",
      "grundsätzliche bedeutung im sinne des § 132 abs. 2 nr. 1\n",
      "\n",
      "freq:  56 length 11\n",
      "die beschwerde der kläger gegen die nichtzulassung der revision in dem\n",
      "\n",
      "freq:  52 length 11\n",
      "in der verwaltungsstreitsache -2- hat der 2. senat des bundesverwaltungsgerichts am\n",
      "\n",
      "freq:  51 length 11\n",
      "in der verwaltungsstreitsache -2- hat der 4. senat des bundesverwaltungsgerichts am\n",
      "\n",
      "freq:  49 length 11\n",
      "in der verwaltungsstreitsache -2- hat der 1. senat des bundesverwaltungsgerichts am\n",
      "\n",
      "freq:  49 length 11\n",
      "in der verwaltungsstreitsache -2- hat der 8. senat des bundesverwaltungsgerichts am\n",
      "\n",
      "freq:  122 length 10\n",
      "als urkundsbeamtin der geschäftsstelle in der verwaltungsstreitsache -2- hat der\n",
      "\n",
      "freq:  96 length 10\n",
      "die kostenentscheidung beruht auf § 154 abs. 2 vwgo, die\n",
      "\n",
      "freq:  90 length 10\n",
      "bedeutung der rechtssache (§ 132 abs. 2 nr. 1 vwgo)\n",
      "\n",
      "freq:  66 length 10\n",
      "in der verwaltungsstreitsache hat der 5. senat des bundesverwaltungsgerichts am\n",
      "\n",
      "freq:  59 length 10\n",
      "in der verwaltungsstreitsache hat der 3. senat des bundesverwaltungsgerichts am\n",
      "\n",
      "freq:  58 length 10\n",
      "den anforderungen des § 133 abs. 3 satz 3 vwgo\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(phrases)):\n",
    "    print(\"freq: \", frequencies[i], \"length\", lengths[i])\n",
    "    print(phrases[i])\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title(\"most common ngrams\")\n",
    "plt.ylabel(\"frequency\")\n",
    "plt.xlabel(\"lenth of ngrams in words\")\n",
    "plt.scatter(lengths, frequencies)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"anzahl dokumente nach verfahrensart für top 6 verfahrensarten. \")\n",
    "subset = df.groupby(['Verfahrensart', 'Entscheidungsart']).size()\n",
    "subset[subset > 200].sort_values().plot(kind='bar')\n",
    "plt.show()\n",
    "count = minidf['text'].str.split(\" \").apply(len).sort_values()\n",
    "plt.xscale(\"log\")\n",
    "count.plot.hist(bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy #load spacy\n",
    "nlp = spacy.load(\"de_core_news_sm\", disable=['parser', 'tagger', 'ner'])\n",
    "#stopwords = stopwords.words(\"german\")\n",
    "\n",
    "def normalize(comment, lowercase, remove_stopwords):\n",
    "    if lowercase:\n",
    "        comment = comment.lower()\n",
    "    comment = nlp(comment)\n",
    "    lemmatized = list()\n",
    "    for word in comment:\n",
    "        lemma = word.lemma_.strip()\n",
    "        if lemma:\n",
    "            if not remove_stopwords or (remove_stopwords and lemma not in stops):\n",
    "                lemmatized.append(lemma)\n",
    "    return \" \".join(lemmatized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.types import StructType,StructField, StringType\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.fpm import PrefixSpan\n",
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext.getOrCreate();\n",
    "plist = minidf['text_normalized'].tolist()\n",
    "rdd = sc.parallelize(plist)\n",
    "#df = spark.createDataFrame(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RDD' object has no attribute '_jdf'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-c7bc5101318f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Find frequent sequential patterns.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprefixSpan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindFrequentSequentialPatterns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/ml/fpm.py\u001b[0m in \u001b[0;36mfindFrequentSequentialPatterns\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindFrequentSequentialPatterns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'RDD' object has no attribute '_jdf'"
     ]
    }
   ],
   "source": [
    "prefixSpan = PrefixSpan(minSupport=0.5, maxPatternLength=20,\n",
    "                        maxLocalProjDBSize=3200000)\n",
    "\n",
    "# Find frequent sequential patterns.\n",
    "prefixSpan.findFrequentSequentialPatterns(rdd).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3, 192.168.2.122, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/zastrow-marcks/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 477, in main\n    (\"%d.%d\" % sys.version_info[:2], version))\nException: Python in worker has different version 2.7 than that in driver 3.8, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:154)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2139)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2120)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2139)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:154)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/zastrow-marcks/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 477, in main\n    (\"%d.%d\" % sys.version_info[:2], version))\nException: Python in worker has different version 2.7 than that in driver 3.8, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:154)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2139)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-bfc0698fc058>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfpm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPrefixSpan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m df = sc.parallelize([Row(sequence=[[1, 2], [3]]),\n\u001b[0m\u001b[1;32m      4\u001b[0m                      \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                      \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mtoDF\u001b[0;34m(self, schema, sampleRatio)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Alice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \"\"\"\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampleRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoDF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    603\u001b[0m             return super(SparkSession, self).createDataFrame(\n\u001b[1;32m    604\u001b[0m                 data, schema, samplingRatio, verifySchema)\n\u001b[0;32m--> 605\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_create_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    629\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_createFromRDD\u001b[0;34m(self, rdd, schema, samplingRatio)\u001b[0m\n\u001b[1;32m    423\u001b[0m         \"\"\"\n\u001b[1;32m    424\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m             \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inferSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m             \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_converter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0mrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_inferSchema\u001b[0;34m(self, rdd, samplingRatio, names)\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStructType\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \"\"\"\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mfirst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfirst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m             raise ValueError(\"The first row in RDD is empty, \"\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mfirst\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1462\u001b[0m         \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRDD\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mempty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1463\u001b[0m         \"\"\"\n\u001b[0;32m-> 1464\u001b[0;31m         \u001b[0mrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1465\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1466\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1446\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1448\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m   1116\u001b[0m         \u001b[0;31m# SparkContext#runJob.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1117\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1118\u001b[0;31m         \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1119\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3, 192.168.2.122, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/zastrow-marcks/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 477, in main\n    (\"%d.%d\" % sys.version_info[:2], version))\nException: Python in worker has different version 2.7 than that in driver 3.8, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:154)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2139)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2120)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2139)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:154)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/zastrow-marcks/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 477, in main\n    (\"%d.%d\" % sys.version_info[:2], version))\nException: Python in worker has different version 2.7 than that in driver 3.8, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:154)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2139)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.fpm import PrefixSpan\n",
    "\n",
    "df = sc.parallelize([Row(sequence=[[1, 2], [3]]),\n",
    "                     Row(sequence=[[1], [3, 2], [1, 2]]),\n",
    "                     Row(sequence=[[1, 2], [5]]),\n",
    "                     Row(sequence=[[6]])]).toDF()\n",
    "\n",
    "prefixSpan = PrefixSpan(minSupport=0.5, maxPatternLength=5)\n",
    "\n",
    "# Find frequent sequential patterns.\n",
    "prefixSpan.findFrequentSequentialPatterns(df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
